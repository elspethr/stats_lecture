{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter estimation: t-tests and p-values\n",
    "\n",
    "One of the most basic data analysis problems is to estimate parameters of a **population**, such as the mean, based on our dataset, which is usually a small **sample** of the population. \n",
    "\n",
    "The characteristics of our sample, such as the mean and standard deviation, are (usually) easy to calculate. But how does our sample relate to the broader population that we want to know about? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A one-sample t-test\n",
    "\n",
    "Imagine you work at a factory where the mean size of a product needs to be 10 cm. For quality control, a sample of 1000 items were measured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  8.94870125  11.6687718    8.75817307  10.61075487  10.41095064\n",
      "  11.04902943   9.36153977   9.16335491   9.82400352  10.74881782]\n"
     ]
    }
   ],
   "source": [
    "#generate a pretend sample \n",
    "import numpy\n",
    "numpy.random.seed(2345)\n",
    "\n",
    "sample = numpy.random.normal(9.9, 1.0, 1000)\n",
    "print sample[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.93712322189\n"
     ]
    }
   ],
   "source": [
    "#what's the mean\n",
    "print numpy.mean(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the mean size of items in the sample is somewhat less than 10 cm. How do we know if this means that the true mean size of the product is less than 10 cm?\n",
    "\n",
    "The classic statistical approach to this problem is a \"one-sample t-test.\" This test basically takes a measure of the distance between the sample mean and the hypothesized mean of the population, and uses statistical distributions as the basis for figuring out how likely it is that our sample came from a population with that mean.\n",
    "\n",
    "To do this, we identify two hypotheses:\n",
    "\n",
    "**$H_0$, the \"null\" hypothesis:** The mean of the sample is NOT DIFFERENT from the expected value.\n",
    "\n",
    "**$H_1$, the \"alternative\" hypothesis:** The mean of the sample is DIFFERENT from the expected value.\n",
    "\n",
    "*Note:* We could ask whether the mean is smaller or larger instead of just different (that's the difference between a one- and two-sided t-test). For now we'll just focus on the two-sided test.\n",
    "\n",
    "In classical statistical testing, we are usually trying to see if there is sufficient evidence *against* the null hypothesis to *reject* it in favour of the alternative. We'll take up the question of *how much* different the samples need to be to say that they are different in a few moments..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the test statistic\n",
    "\n",
    "The basic equation for a one-sample t-test is:\n",
    "\n",
    "$t=\\frac{\\bar{x}-\\mu}{\\sigma/\\sqrt{n}}$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\\bar{x}$ is the sample mean\n",
    "\n",
    "- $\\mu$ is the mean value for the population (in our case the expected population mean)\n",
    "\n",
    "- $\\sigma$ is the standard deviation of the sample\n",
    "\n",
    "- $n$ is the size of the sample\n",
    "\n",
    "So in our example data, the value of $t$ is: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t-statistic = -2.035\n"
     ]
    }
   ],
   "source": [
    "tstat = (numpy.mean(sample)-10)/(numpy.std(sample)/numpy.sqrt(1000))\n",
    "print \"t-statistic = %.3f\" %tstat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A couple important notes before we move on to interpreting what this test statistic means. \n",
    "\n",
    "First, the $t$-test has a few key assumptions:\n",
    "\n",
    "- the observations in the sample are independent (often taken to mean that they should have been randomly selected from the population)\n",
    "- the population from which the sample was drawn should be approximately normally distributed, or rather, the distribution of sample means should be normal\n",
    "\n",
    "Further, $t$ differs from a $Z$-score because we are using the sample standard deviation, adjusted by the size of the sample, to scale the data instead of the population standard deviation (if you knew the population standard deviation, you should do a $Z$-test!). Because of the greater uncertainty caused by using the sample standard deviation as an approximation, we use a sampling distribution broader than a standard normal distribution to interpret the $t$ test statistic. However, as the sample size $n$ increases, the $t$-distribution approaches normal (remember the central limit theorem). What this means is that the sample size enters in the calculation of the significance of the test (through the degrees of freedom), which we'll get to next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the test statistic: calculating $p$\n",
    "\n",
    "The $t$ statistic measured earlier is a measure of how far the observed sample mean is from our hypothetical population mean. To decide whether or not this distance is large enough that we should reject the null hypothesis, we'll use the concept of a \"p-value.\"\n",
    "\n",
    "The **p-value** is defined as the probability of obtaining a result equal to or \"more extreme\" than what was actually observed, when the null hypothesis is true.\n",
    "\n",
    "For our test case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p-value = 0.0421\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import t\n",
    "\n",
    "tstat\n",
    "df = 1000-1\n",
    "\n",
    "pval = t.sf(numpy.abs(tstat), df)*2  # two-sided pvalue = Prob(abs(t)>tt)\n",
    "print 'p-value = %.4f' %(pval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#should put a plot illustrating the result here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, if the null hypothesis is true, and the true mean of the population is 10 cm, then the probability of observing a sample with the mean in our sample is about 4%. So, either the true mean of the population that generated this sample is NOT 10 cm, or we simply selected a pretty unusual sample (hence the importance of independence in our sample!)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the test statistic: determining significance\n",
    "\n",
    "Now, finally, we need to decide whether this outcome is sufficiently unlikely that we should reject the null. To do this, we need a couple more concepts.\n",
    "\n",
    "First, think about the four possible outcomes of a test:\n",
    "\n",
    "|   |   | Prediction | |\n",
    "|---|---|---|---|\n",
    "|       |    | True  | False |\n",
    "|**Reality**    | True  | True positive (TP) | False negative (FN)|\n",
    "|           | False |  False positive (FP)| True negative (TN) |\n",
    "\n",
    "In statistical hypothesis testing, we refer to the \"bad outcomes\" (a test that leads to be a FN or FP outcome) as two different types of error:\n",
    "\n",
    "**Type I error:** FP, or rejecting the null when it is true\n",
    "\n",
    "**Type II error:** FN, or accepting the null when it is false\n",
    "\n",
    "We refer to the probability of a Type I error that we're willing to accept as $\\alpha$ (the \"significance level\").  By convention, this is often set at 0.05 or 0.01. This is the probability of making the wrong decision *when the null hypothesis is true.*\n",
    "\n",
    "The null hypothesis we specified earlier was that the population mean was 10 cm. We have found that the probability of observing a sample as extreme as ours is about 0.04. \n",
    "\n",
    "So, in our example case, if we take $\\alpha$ = 0.05, since our $p$-value (0.04) is less than $\\alpha$, we could reject the null. It seems unlikely that our sample came from a population with a true mean of 10 cm. But you should choose your significance level BEFORE conducting your test!\n",
    "\n",
    "*Important:* $p$-values are related to the concept of Type I error but are not the actual Type I error rate of a model! Further, they SAY NOTHING about Type II error (power, or \"sensitivity,\" analysis is required to examine the probability of Type II error). Beware!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "Some key things to remember about $p$-values:\n",
    "\n",
    "- a low $p$-value simply implies that either (a) the null is true and you've got a very unlikely sample, or (b) that the null is false\n",
    "\n",
    "- they say nothing about the relative probabilities of the hypotheses considered, in fact the test says nothing about the probability of the alternative being true.\n",
    "\n",
    "- statistical significance is not the same as practical significance!\n",
    "\n",
    "This hypothesis testing approach is the basis of most classic statistical tests. This approach is often referred to as \"frequentist\" statistics, having to do with the idea of how \"frequent\" the observed outcome would be if you repeated the \"experiment\" many times. If this approach seems unsatisfying to you---perhaps because the choice of significance level seems arbitrary or because you think we should actually figure out how likely the alternative hypothesis is---you are not alone. We'll discuss an alternative approach in the next notebook (Parameter_estimation_bayesian)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding model performance\n",
    "\n",
    "Type I and Type II error are closely related to measures used to assess the performance of classification models:\n",
    "\n",
    "**Precision:** Proportion of predicted positives that are truly positive. $\\frac{TP}{TP+FP}$\n",
    "\n",
    "**Recall:** Proportion of true positives that are correctly predicted. $\\frac{TP}{TP+FN}$\n",
    "\n",
    "Thus, Type I error (the probability of a test being a false positive) is related to precision, while Type II error (the probability of a test being a false negative) is related to recall.\n",
    "\n",
    "*NB:* Sensitivity = recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
